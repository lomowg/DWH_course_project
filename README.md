# Практическая реализация DWH для телеком-компании (CSV → PostgreSQL → BI)

## 1) Краткое описание проекта

Проект демонстрирует прототип хранилища данных (DWH) телекоммуникационной компании по схеме «звезда». В рамках практики выполняются:

* генерация тестовых данных в формате **CSV** (абоненты, тарифы, услуги, платежи, начисления, usage/CDR и сетевые KPI).
* загрузка данных в **PostgreSQL** через ETL (staging во временные таблицы → измерения → факты).
* создание **представлений (витрин)** для BI-отчётов (коммерческие KPI, churn, сеть/SLA).

![img.png](img.png)
---

## 2) Структура проекта

```
KR/
 ├─ Core_tables.sql                  # создание таблиц DWH (измерения/факты)
 ├─ Bi_views.sql                     # представления (витрины) для BI
 ├─ Generate_test_data.py            # генерация CSV в папку data_out/
 ├─ ETL.py                           # ETL: загрузка CSV → PostgreSQL
 ├─ data_out/                        # результат генерации CSV
 │   ├─ subscribers.csv
 │   ├─ tariffs.csv
 │   ├─ services.csv
 │   ├─ channels.csv
 │   ├─ cell_sites.csv
 │   ├─ usage.csv
 │   ├─ billing.csv
 │   ├─ payments.csv
 │   └─ network_kpi.csv
 └─ Docs/
     ├─ ER-диаграмма.pdf
     └─ Описание таблиц.pdf
```

### Папка `Docs`

Это папка с описанием таблиц и базы данных.

#### `ER-диаграмма.pdf`
Используется как визуальная схема физической модели (таблицы + связи + ключи).

#### `Описание таблиц.pdf`
Текстовое описание таблиц: назначение, ключи, основные поля, связи и назначение фактов/измерений.


## 3) Подготовка PostgreSQL

1. Создайте базу данных (пример: `KR`)
2. Убедитесь, что пользователь имеет права на создание таблиц и представлений.

Пример SQL (в psql/pgAdmin):

```sql
CREATE DATABASE "KR";
```

---

## 4) Настройка подключения

### В файле ETL.py

Поменять данные для подключения. По умолчанию: `localhost:5432`, база `KR`, пользователь `postgres`, пароль `root`

---

## 5) Установка зависимостей Python

В папке проекта:

```bash
pip install psycopg2-binary
```

---

## 6) Запуск

### Шаг 1. Генерация тестовых данных (CSV)

Запустить:

```bash
python Generate_test_data.py 
```

Результат: в папке `data_out/` появятся CSV-файлы.

---

### Шаг 2. Загрузка данных в PostgreSQL (ETL)

Запустить:

```bash
python ETL.py
```

Что делает ETL:

1. выполняет `Core_tables.sql` (создаёт таблицы, если их нет);
2. очищает таблицы (TRUNCATE … CASCADE);
3. создаёт временные staging-таблицы `tmp_*`;
4. загружает CSV в `tmp_*` через `COPY`;
5. заполняет `dim_date` и `dim_time` на основе диапазона дат в staging;
6. загружает измерения (`dim_*`);
7. загружает факты (`fact_*`);
8. создаёт витрины/представления из `Bi_views.sql`;
9. выводит в консоль количество строк в фактах.

---

## 7) Проверка результата (SQL)

После ETL можно выполнить:

```sql
SELECT COUNT(*) FROM fact_usage;
SELECT MIN(full_date), MAX(full_date), COUNT(*) FROM dim_date;

SELECT * FROM v_churn_monthly ORDER BY year, month LIMIT 12;
SELECT * FROM v_network_daily ORDER BY date DESC LIMIT 20;
```

---
